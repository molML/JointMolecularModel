training_config:
  experiment_name: 'pretrain_default'
  out_path: 'results'
  draw_mol: False
  num_workers: 1
  max_iters: 50000
  batch_size: 256
  batch_end_callback_every: 500
  save_every: null
  val_molecules_to_sample: 1000
  early_stopping_patience: 5
  early_stopping_metric: 'edit_distance'
  early_stopping_eps: 0.01
  early_stopping_should_go_down: True
  lr_scheduler_gamma: 0.99995
  stop_reducing_lr_after: 100000
  descriptor: 'smiles'
hyperparameters:
  lr: 0.0003
  kernel_size: 8
  hidden_dim_lstm: 512
  hidden_dim_cnn: 256
  n_layers_cnn: 2
  learnable_cell_state: False
  teacher_forcing_prob: 1
  variational_scale: 0.01
  start_token_weight: 10
  beta: 0.001
  vocab_size: 36
  latent_dim: 128
  seq_length: 102
  grad_norm_clip: null
  device: 'auto'
